{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_Product(PhoBert_base).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rk60B5uRZLw6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "58190058ac304aaaa857d2efed0e7591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_046c265707864e67b9f25db9fcabe51b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9203cdc9126f4bcc9bd2828238a81815",
              "IPY_MODEL_7a694c7b453b411f915fc5a505dcec77"
            ]
          }
        },
        "046c265707864e67b9f25db9fcabe51b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9203cdc9126f4bcc9bd2828238a81815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1f9e0876ba2e4611be97d1b1589396d9",
            "_dom_classes": [],
            "description": "Epoch: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e389baedbdf04f05ab5cb908ee2226e7"
          }
        },
        "7a694c7b453b411f915fc5a505dcec77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e4e1b0f5934417fad1423ef3f12bfcf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [03:12&lt;00:00, 38.41s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7f94893066746bd87f791fa00cd7812"
          }
        },
        "1f9e0876ba2e4611be97d1b1589396d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e389baedbdf04f05ab5cb908ee2226e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e4e1b0f5934417fad1423ef3f12bfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7f94893066746bd87f791fa00cd7812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqFZ9S8iYjAt",
        "colab_type": "text"
      },
      "source": [
        "#Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTpTqgffYdFj",
        "colab_type": "code",
        "outputId": "8a190778-d797-4202-f10a-1dfb90d756b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        }
      },
      "source": [
        "!pip install transformers==2.6.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a0/32e3a4501ef480f7ea01aac329a716132f32f7911ef1c2fac228acc57ca7/transformers-2.6.0-py3-none-any.whl (540kB)\n",
            "\r\u001b[K     |▋                               | 10kB 28.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 409kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 481kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 501kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 4.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 22.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (1.13.10)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.6.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 21.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.6.0) (2.9)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (1.16.10)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.6.0) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.6.0) (0.15.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.10->boto3->transformers==2.6.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.10->boto3->transformers==2.6.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ed9e2a7f5c7a90c8e4b0659143e838f409f8ee998acfb29299e53fefcc761287\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.5.2 transformers-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egqq-nMHYtDG",
        "colab_type": "code",
        "outputId": "c123c85e-6b73-4390-9236-38feb2428e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/bf/de299e082e7af010d35162cb9a185dc6c17db71624590f2f379aeb2519ff/fairseq-0.9.0.tar.gz (306kB)\n",
            "\r\u001b[K     |█                               | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp36-cp36m-linux_x86_64.whl size=2035354 sha256=6b340016bf721f682161484d2f73dbdb178981c5b39302e2b3fcda4da0cfff3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/3e/1b/0fa30695dcba41e4b0088067fa40f3328d1e8ee78c22cd4766\n",
            "Successfully built fairseq\n",
            "Installing collected packages: portalocker, sacrebleu, fairseq\n",
            "Successfully installed fairseq-0.9.0 portalocker-1.7.0 sacrebleu-1.4.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxPmfRKEYvWM",
        "colab_type": "code",
        "outputId": "b4e4f90b-98b8-4960-83f4-913c3a00828b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!pip install fastBPE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastBPE\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
            "Building wheels for collected packages: fastBPE\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=479265 sha256=b128e7deab499fdf81cde93603f13a724d15c73c302c80e1d8cd2c942eb6de53\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
            "Successfully built fastBPE\n",
            "Installing collected packages: fastBPE\n",
            "Successfully installed fastBPE-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZBCwr9NdWhS",
        "colab_type": "code",
        "outputId": "0702d42a-e576-4550-8b2b-837c6769fe2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.4)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=bf62c637038b6e61d67e75033a745bf8b185cb8253400d3be5c6f62c9e05594e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6xCcDtbY3f0",
        "colab_type": "code",
        "outputId": "746233ee-bedd-4af0-c235-16ba13a6f59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "import pandas as pd\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "\n",
        "from transformers import RobertaConfig, RobertaForTokenClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import trange\n",
        "from seqeval.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdv00jsYfhd0",
        "colab_type": "code",
        "outputId": "cc8e3f1d-aaf6-4498-f557-9bd6698a0667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHPeu0KffDOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 60\n",
        "batch_sz = 16\n",
        "epochs = 5\n",
        "max_grad_norm = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TpMivmBw95V",
        "colab_type": "code",
        "outputId": "8475882e-8c68-44cd-e941-658be39a6c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print('{}: {}'.format(device, n_gpu))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrQh9wLOfJpC",
        "colab_type": "text"
      },
      "source": [
        "#Load Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EO5x0SxoorZ",
        "colab_type": "text"
      },
      "source": [
        "**Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcgSUnfPfIxT",
        "colab_type": "code",
        "outputId": "ffa59790-08c9-47dc-8c01-300979040dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/data/ner_products_bio.csv', header=None,\n",
        "                   sep='\\t', encoding='utf8', names=['Sentence#', 'Word', 'Tag'])\n",
        "data.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence#</th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Nhưng</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>hiện_tại</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>bạn</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>đang</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>cho</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>bé</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>bú</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>nên</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>bạn</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>không</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentence#      Word Tag\n",
              "0          1     Nhưng   O\n",
              "1          1  hiện_tại   O\n",
              "2          1       bạn   O\n",
              "3          1      đang   O\n",
              "4          1       cho   O\n",
              "5          1        bé   O\n",
              "6          1        bú   O\n",
              "7          1       nên   O\n",
              "8          1       bạn   O\n",
              "9          1     không   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZZOEJcNioYq",
        "colab_type": "code",
        "outputId": "a94403d5-c605-4b68-d3cf-78a2c4374625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "tuple_func = lambda f: [(w, t) for w, t in zip(f['Word'].values, f['Tag'].values)]\n",
        "sentences_with_tag = data.groupby('Sentence#').apply(tuple_func)\n",
        "print(sentences_with_tag)\n",
        "sentences_with_tag = [sent for sent in sentences_with_tag]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence#\n",
            "1       [(Nhưng, O), (hiện_tại, O), (bạn, O), (đang, O...\n",
            "2                        [(Áo, B-pr), (Hm, I-pr), (ạ, O)]\n",
            "3       [(Cho, O), (mình, O), (đặt, O), (cái, O), (bal...\n",
            "4       [(Ac, O), (có, O), (vest, B-pr), (chưa, O), (?...\n",
            "5       [(body, B-pr), (lotion, I-pr), (e, O), (lại, O...\n",
            "                              ...                        \n",
            "2949    [(hiện_tại, O), (ở, O), (thành_phố, O), (cao_l...\n",
            "2950    [(10, B-pr), (+, I-pr), (có, O), (hỗ_trợ, O), ...\n",
            "2951    [(Tôi, O), (có, O), (mua, O), (1, O), (note, B...\n",
            "2952    [(Mình, O), (có, O), (samsung, B-pr), (S7edge,...\n",
            "2953    [(Ad, O), (cko, B-pr), (hoi, O), (gia, O), (hi...\n",
            "Length: 2952, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r60imrFdkOpJ",
        "colab_type": "code",
        "outputId": "c640f4f3-c7c1-4aa3-94bd-6435ce11e6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences = [' '.join([word[0] for word in sent]) for sent in sentences_with_tag]\n",
        "sentences[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Áo Hm ạ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv4hl9q1kpLq",
        "colab_type": "code",
        "outputId": "efe0a327-4ab2-41b5-e38a-d6b37bef7953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels = [[word[1] for word in sent] for sent in sentences_with_tag]\n",
        "labels[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-pr', 'I-pr', 'O']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYmGJG8wk_MM",
        "colab_type": "code",
        "outputId": "31a7f003-dd21-484b-fe4f-022a82f64176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels_value = ['B-pr','I-pr','O', 'PAD', '[CLS]', '[SEP]', 'X']\n",
        "label2idx = {label:indx for indx, label in enumerate(labels_value)}\n",
        "label2idx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-pr': 0, 'I-pr': 1, 'O': 2, 'PAD': 3, 'X': 6, '[CLS]': 4, '[SEP]': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq7o6689grKS",
        "colab_type": "text"
      },
      "source": [
        "***Encode with bpe***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRffu0CnmGZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--bpe-codes', \n",
        "    default=\"/content/drive/My Drive/pre_model/phobert/PhoBERT_base_transformers/bpe.codes\",\n",
        "    required=False,\n",
        "    type=str,  \n",
        "    help='path to fastBPE BPE'\n",
        ")\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "bpe = fastBPE(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FchxuuOUmgd5",
        "colab_type": "code",
        "outputId": "3ff5acf6-9f7b-4a8f-ac1d-9bbcf91c5763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "subwords = ['<s> '+bpe.encode(sent)+' </s>' for sent in sentences]\n",
        "subwords[1] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> Áo H@@ m ạ </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ65kPhdnS3k",
        "colab_type": "code",
        "outputId": "0f14d3e0-c31d-48ec-adf8-e5caf660c923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.hist([len(s.split()) for s in subwords])\n",
        "plt.xlabel('Number of word')\n",
        "plt.ylabel('Number of sentences')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaS_WRMKoPz9",
        "colab_type": "text"
      },
      "source": [
        "***Change labels by subword***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY3O9SFPn2lQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_subwords = []\n",
        "for i in range(len(subwords)):\n",
        "  indx = 0\n",
        "  label = ['[CLS]']\n",
        "  for word in subwords[i].split()[1:MAX_LEN-1]:\n",
        "    if word == '</s>':\n",
        "      break\n",
        "    if '@@' not in word:\n",
        "      label.append(labels[i][indx])\n",
        "      indx += 1\n",
        "      continue\n",
        "    label.append('X')\n",
        "  label.append('[SEP]')\n",
        "  label_subwords.append(label) \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJWHuchWyRd6",
        "colab_type": "code",
        "outputId": "aa692a55-6cb2-41f9-a7a2-fb75db33e9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "label_subwords[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'B-pr', 'X', 'I-pr', 'O', '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnoorgFJhvav",
        "colab_type": "text"
      },
      "source": [
        "***String to number***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtM9Wg6kmeMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"/content/drive/My Drive/pre_model/phobert/PhoBERT_large_transformers/dict.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj3CxWSknK4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = pad_sequences([vocab.encode_line(sent, append_eos=False, add_if_not_exist=False).long().tolist() for sent in subwords],\n",
        "                          truncating='post', padding='post', maxlen=MAX_LEN, value=1.0, dtype='long')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spjCT1eljz6v",
        "colab_type": "code",
        "outputId": "214a8e90-7056-476d-8377-9780f58661ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "input_ids[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0, 3759, 1125,  599, 3628,    2,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "          1,    1,    1,    1,    1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGiwadUIkHLU",
        "colab_type": "code",
        "outputId": "2bad79f2-f376-4a1a-8c27-22af7f6f8c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "labels_ids = pad_sequences([[label2idx.get(label) for label in labels] for labels in label_subwords], dtype='long',\n",
        "                            maxlen=MAX_LEN, value=label2idx['PAD'], truncating='post', padding='post')\n",
        "labels_ids[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 0, 6, 1, 2, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6nNRWHlNbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attenion_mask = [[float(val != 1) for val in sent] for sent in input_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ajISqfmBUF",
        "colab_type": "text"
      },
      "source": [
        "***Create train/validation data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAD9zrmiTNAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = input_ids[2592:]\n",
        "y_test = labels_ids[2592:]\n",
        "test_mask = attenion_mask[2592:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JteRr4K7l6rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(input_ids[:2592], labels_ids[:2592], random_state=96, test_size=0.2)\n",
        "train_mask, val_mask, _, _ = train_test_split(attenion_mask[:2592], input_ids[:2592], random_state=96, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e6zMj1im3Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change to tensor\n",
        "X_train = torch.tensor(X_train)\n",
        "X_val = torch.tensor(X_val)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_train = torch.tensor(y_train)\n",
        "y_val = torch.tensor(y_val)\n",
        "y_test = torch.tensor(y_test)\n",
        "train_mask = torch.tensor(train_mask)\n",
        "val_mask = torch.tensor(val_mask)\n",
        "test_mask = torch.tensor(test_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQHBYd-vnTiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create data loader\n",
        "train_data = TensorDataset(X_train, train_mask, y_train)\n",
        "train_sample = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sample, batch_size=batch_sz)\n",
        "\n",
        "val_data = TensorDataset(X_val, val_mask, y_val)\n",
        "val_sample = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sample, batch_size=batch_sz)\n",
        "\n",
        "test_data = TensorDataset(X_test, test_mask, y_test)\n",
        "test_sample = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sample, batch_size=batch_sz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9kIr1uRopx_",
        "colab_type": "code",
        "outputId": "20557d64-7fa7-4399-a4f8-2e5b33204448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "test_data[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([   0, 1631,    8, 3214, 1529,  431,   51, 3438, 1204,   68,   60, 4294,\n",
              "         2789, 1340, 3760,    2,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]),\n",
              " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.]),\n",
              " tensor([4, 2, 2, 6, 6, 2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 5, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMlEkVv_qP3N",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqIBBDftqU5c",
        "colab_type": "text"
      },
      "source": [
        "***Load pretrained model PhoBert(large)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIiDPmpNHAyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import *\n",
        "\n",
        "class Ner(BertPreTrainedModel):\n",
        "   config_class = RobertaConfig\n",
        "   pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
        "   base_model_prefix = \"roberta\"\n",
        "   def __init__(self, config):\n",
        "       super().__init__(config)\n",
        "       self.num_labels = config.num_labels\n",
        "       self.roberta = RobertaModel(config)\n",
        "       self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "       self.classifier = nn.Linear(config.hidden_size*4, config.num_labels)\n",
        "       self.init_weights()\n",
        "\n",
        "   def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "       outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "       \n",
        "       output = torch.cat((outputs[2][-1],outputs[2][-2], outputs[2][-3], outputs[2][-4]), dim=-1)\n",
        "       sequence_output = self.dropout(output)\n",
        "       logits = self.classifier(sequence_output)\n",
        "       outputs = logits\n",
        "\n",
        "       if labels is not None:\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        active_loss = attention_mask.view(-1) == 1\n",
        "        active_logits = logits.view(-1, self.num_labels)\n",
        "        active_labels = torch.where(\n",
        "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "        )\n",
        "        loss = loss_fct(active_logits, active_labels)\n",
        "        outputs = (loss, logits)\n",
        "       return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vFvMGKsqFrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = RobertaConfig.from_pretrained(\n",
        "    \"/content/drive/My Drive/pre_model/phobert/PhoBERT_base_transformers/config.json\",\n",
        "    num_labels=len(label2idx),\n",
        "    output_hidden_states=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3NclR75qcj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RobertaForTokenClassification.from_pretrained(\n",
        "    \"/content/drive/My Drive/pre_model/phobert/PhoBERT_base_transformers/model.bin\",\n",
        "    config=config\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8IOLp9aJDW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Ner.from_pretrained(\n",
        "    \"/content/drive/My Drive/pre_model/phobert/PhoBERT_base_transformers/model.bin\",\n",
        "    config=config\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nFbgfC3qpRE",
        "colab_type": "code",
        "outputId": "a96e2f24-f9e5-49bd-cdbd-33974417435a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# pass model parameter to GPU\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ner(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=3072, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foDkfN9vqsuU",
        "colab_type": "text"
      },
      "source": [
        "***Add optimizer***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD3h4YHJqqzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "    'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "    'weight_decay_rate': 0.0}\n",
        "]\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOvsF0rjuCgg",
        "colab_type": "text"
      },
      "source": [
        "***Add a scheduler to linearly reduce the learning rate throughout the epochs***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMRZMD6cr96X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_steps = len(train_dataloader)*epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKGguI5MwRQX",
        "colab_type": "text"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqMPxWvqpCj2",
        "colab_type": "code",
        "outputId": "e54f1bce-14eb-44ff-f321-23410eeb4eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "58190058ac304aaaa857d2efed0e7591",
            "046c265707864e67b9f25db9fcabe51b",
            "9203cdc9126f4bcc9bd2828238a81815",
            "7a694c7b453b411f915fc5a505dcec77",
            "1f9e0876ba2e4611be97d1b1589396d9",
            "e389baedbdf04f05ab5cb908ee2226e7",
            "4e4e1b0f5934417fad1423ef3f12bfcf",
            "b7f94893066746bd87f791fa00cd7812"
          ]
        }
      },
      "source": [
        "for epoch in trange(epochs, desc='Epoch'):\n",
        "  \n",
        "  # TRAIN\n",
        "  \n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for batch in train_dataloader:\n",
        "    # add to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    X_train, train_mask, y_train = batch\n",
        "\n",
        "    # clear gradient after each backward\n",
        "    model.zero_grad()\n",
        "\n",
        "    # get loss and score\n",
        "    output = model(X_train, attention_mask=train_mask, labels=y_train)\n",
        "\n",
        "    loss = output[0]\n",
        "    loss.backward()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    # norm gradient\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "    \n",
        "    # update parameter\n",
        "    optimizer.step()\n",
        "    # update learning rate\n",
        "    scheduler.step()\n",
        "  \n",
        "  print('Average train loss: {}'.format(total_loss/len(train_dataloader)))\n",
        "\n",
        "  # VALIDATION\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  pred_labels_ids, true_labels_ids, val_ids_sent = [], [], []\n",
        "  for batch in val_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    X_val, val_mask, y_val = batch\n",
        "\n",
        "    # return (score)\n",
        "    with torch.no_grad():\n",
        "      output_val = model(X_val, val_mask)\n",
        "    \n",
        "    # move to CPU\n",
        "    val_ids_sent.extend(X_val)\n",
        "    logit = output_val.detach().cpu().numpy()\n",
        "    label_ids = y_val.to('cpu').numpy()\n",
        "\n",
        "    pred_labels_ids.extend([list(pred_label) for pred_label in np.argmax(logit, axis=2)])\n",
        "    true_labels_ids.extend(label_ids)\n",
        "  \n",
        "  # dont use PAD labels to compute f1 score\n",
        "  pred_labels = [labels_value[pred_indx] for pred, true in zip(pred_labels_ids, true_labels_ids)\n",
        "                                      for pred_indx, true_indx in zip(pred, true) if labels_value[true_indx] != 'PAD']\n",
        "  true_labels = [labels_value[indx] for true in true_labels_ids\n",
        "                                      for indx in true if labels_value[indx] != 'PAD']\n",
        "  print('{}. Validation F1-score: {}\\n'.format(epoch+1, f1_score(pred_labels, true_labels)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58190058ac304aaaa857d2efed0e7591",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Average train loss: 0.38393247563105365\n",
            "1. Validation F1-score: 0.9129989764585467\n",
            "\n",
            "Average train loss: 0.12224725241271349\n",
            "2. Validation F1-score: 0.9247223364870424\n",
            "\n",
            "Average train loss: 0.08897574692964554\n",
            "3. Validation F1-score: 0.9324903120538445\n",
            "\n",
            "Average train loss: 0.06506329245435503\n",
            "4. Validation F1-score: 0.9339333196972797\n",
            "\n",
            "Average train loss: 0.05193760571284936\n",
            "5. Validation F1-score: 0.9347333745110151\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhXBySlHfyJm",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebN-f9Pxe7yc",
        "colab_type": "code",
        "outputId": "a4ca1256-5544-4054-b7e3-93468b2346dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pred_labels_ids, true_labels_ids, val_ids_sent = [], [], []\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    X, mask, y = batch\n",
        "\n",
        "    # return (score)\n",
        "    with torch.no_grad():\n",
        "      output_test = model(X, mask)\n",
        "    \n",
        "    # move to CPU\n",
        "    val_ids_sent.extend(X)\n",
        "    logit = output_test.detach().cpu().numpy()\n",
        "    label_ids = y.to('cpu').numpy()\n",
        "\n",
        "    pred_labels_ids.extend([list(pred_label) for pred_label in np.argmax(logit, axis=2)])\n",
        "    true_labels_ids.extend(label_ids)\n",
        "  \n",
        "  # dont use PAD labels to compute f1 score\n",
        "pred_labels = [labels_value[pred_indx] for pred, true in zip(pred_labels_ids, true_labels_ids)\n",
        "                                      for pred_indx, true_indx in zip(pred, true) if labels_value[true_indx] != 'PAD']\n",
        "true_labels = [labels_value[indx] for true in true_labels_ids\n",
        "                                      for indx in true if labels_value[indx] != 'PAD']\n",
        "print('Test F1-score: {}\\n'.format(f1_score(pred_labels, true_labels)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test F1-score: 0.9660163624921334\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdDH_kk4oDOb",
        "colab_type": "code",
        "outputId": "695ecd18-5b41-411c-a48e-b005d31566b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "list(zip(true_labels, pred_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'O'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'I-pr'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'I-pr'),\n",
              " ('O', 'I-pr'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'O'),\n",
              " ('I-pr', 'O'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'O'),\n",
              " ('I-pr', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('X', 'X'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('X', 'X'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('B-pr', 'B-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('I-pr', 'I-pr'),\n",
              " ('O', 'O'),\n",
              " ('X', 'X'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('[SEP]', '[SEP]'),\n",
              " ('[CLS]', '[CLS]'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ('O', 'O'),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnFUKtkig0Ss",
        "colab_type": "text"
      },
      "source": [
        "*Model dự đoán gần như là chính xác hoàn toàn. Có những từ bị đánh label thiếu trong tập data, model cũng dự đoán được. Tuy nhiên model nhận dạng khá không đầy đủ tên của những sản phẩm có thuộc tính đi kèm, ví dụ:*\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  True                      Predict\n",
        "\n",
        "Chuột logitech k dây --> Chuột logitech\n",
        "Đầm thun hoa dáng dài --> Đầm thun hoa\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk60B5uRZLw6",
        "colab_type": "text"
      },
      "source": [
        "#Save & load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymwxGm4rbtE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = '/content/drive/My Drive/pre_model/phobert/ner_product/model_base_state_dict.pt'\n",
        "labels_value = ['B-pr','I-pr','O', 'PAD', '[CLS]', '[SEP]', 'X']\n",
        "label2idx = {label:indx for indx, label in enumerate(labels_value)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqOOUeNnwtGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(),PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltK9QuJI3llx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ner = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjblyC4Lbj67",
        "colab_type": "code",
        "outputId": "96a36286-6c42-4f0c-eb85-0d01b5cd917c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ner.load_state_dict(torch.load(PATH))\n",
        "ner.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ner(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=3072, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2HOIe277Vly",
        "colab_type": "text"
      },
      "source": [
        "#Test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOq-cl3zC817",
        "colab_type": "text"
      },
      "source": [
        "***Must use VnCoreNLP to tokenize before test***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sf-gL-L7aKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentence = 'Cho mình hỏi oppo reno 3 có ghi_âm cuộc_gọi không ạk?'\n",
        "sent = '<s> ' + bpe.encode(test_sentence) +' </s>'\n",
        "sent_ids = vocab.encode_line(sent, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "\n",
        "tokenized_sentence = pad_sequences([sent_ids], maxlen=MAX_LEN, dtype=\"long\", value=1.0, truncating=\"post\", padding=\"post\")\n",
        "input_ids = torch.tensor(tokenized_sentence)\n",
        "\n",
        "mask = [[float(m != 1) for m in val] for val in input_ids]\n",
        "mask = torch.tensor(mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = ner(input_ids.cuda(), mask.cuda())\n",
        "label_ids = np.argmax(output[0].cpu().numpy(), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0-Qw0BdYfAT",
        "colab_type": "code",
        "outputId": "8932e6df-fcdc-405e-f526-339170ae1bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "subword_label = list(zip([labels_value[label] for label in label_ids], sent.split()))\n",
        "subword_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[CLS]', '<s>'),\n",
              " ('O', 'Cho'),\n",
              " ('O', 'mình'),\n",
              " ('O', 'hỏi'),\n",
              " ('X', 'o@@'),\n",
              " ('B-pr', 'ppo'),\n",
              " ('X', 'ren@@'),\n",
              " ('I-pr', 'o'),\n",
              " ('I-pr', '3'),\n",
              " ('O', 'có'),\n",
              " ('O', 'ghi_âm'),\n",
              " ('O', 'cuộc_gọi'),\n",
              " ('O', 'không'),\n",
              " ('X', 'ạ@@'),\n",
              " ('X', 'k@@'),\n",
              " ('O', '?'),\n",
              " ('[SEP]', '</s>')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx2d8MTsYz7c",
        "colab_type": "code",
        "outputId": "44691946-51c6-4237-c93c-d4b5dfba8256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "sentence = []\n",
        "labels = []\n",
        "word = ''\n",
        "for label, sword in subword_label:\n",
        "  if '@@' not in sword:\n",
        "    word += sword\n",
        "    sentence.append(word)\n",
        "    labels.append(label)\n",
        "    word = ''\n",
        "    continue\n",
        "  word += sword.replace('@@', '')\n",
        "\n",
        "list(zip(sentence, labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', '[CLS]'),\n",
              " ('Cho', 'O'),\n",
              " ('mình', 'O'),\n",
              " ('hỏi', 'O'),\n",
              " ('oppo', 'B-pr'),\n",
              " ('reno', 'I-pr'),\n",
              " ('3', 'I-pr'),\n",
              " ('có', 'O'),\n",
              " ('ghi_âm', 'O'),\n",
              " ('cuộc_gọi', 'O'),\n",
              " ('không', 'O'),\n",
              " ('ạk?', 'O'),\n",
              " ('</s>', '[SEP]')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1AuVZ-ymK3f",
        "colab_type": "code",
        "outputId": "d9437555-824c-4fd4-87ff-e14cf8055583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Product perform by upper word\n",
        "sentence2string = ''\n",
        "for word, label in list(zip(sentence, labels))[1:len(sentence)-1]:\n",
        "  if label == 'O':\n",
        "    sentence2string += (word.lower() +' ')\n",
        "    continue\n",
        "  sentence2string += (word.upper() +' ')\n",
        "sentence2string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cho mình hỏi OPPO RENO 3 có ghi_âm cuộc_gọi không ạk? '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsIWRj9Mw1HW",
        "colab_type": "text"
      },
      "source": [
        "#Load test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX6aTV7fK_lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PredictProduct:\n",
        "  def __init__(self):\n",
        "    self.vocab = self.load_vocab()\n",
        "    self.bpe = self.load_bpe()\n",
        "    self.data = self.load_data()\n",
        "  \n",
        "  def load_vocab(self):\n",
        "    vocab = Dictionary()\n",
        "    vocab.add_from_file(\"/content/drive/My Drive/pre_model/phobert/PhoBERT_large_transformers/dict.txt\")\n",
        "    \n",
        "    return vocab\n",
        "  \n",
        "  def load_bpe(self):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--bpe-codes', \n",
        "      default=\"/content/drive/My Drive/pre_model/phobert/PhoBERT_base_transformers/bpe.codes\",\n",
        "      required=False,\n",
        "      type=str,  \n",
        "      help='path to fastBPE BPE'\n",
        "    )\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    bpe = fastBPE(args)\n",
        "\n",
        "    return bpe\n",
        "  \n",
        "  def load_data(self):\n",
        "    sents, sent_ids, masks = [], [], []\n",
        "    with open('/content/drive/My Drive/data/note.txt', encoding='utf8') as fr:\n",
        "      for line in fr:\n",
        "        sent = '<s> ' + self.bpe.encode(line) +' </s>'\n",
        "        sent2ids = self.vocab.encode_line(sent, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "        tokenized_sentence = pad_sequences([sent2ids], maxlen=MAX_LEN, dtype=\"long\", value=1.0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "        mask = [[float(m != 1) for m in val] for val in tokenized_sentence]\n",
        "\n",
        "        sents.append(sent)\n",
        "        sent_ids.append(tokenized_sentence)\n",
        "        masks.append(mask)\n",
        "\n",
        "    sent_ids = torch.tensor(sent_ids)\n",
        "    masks = torch.tensor(masks)\n",
        "    return (sents, sent_ids, masks)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAbYXU6q25zL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = PredictProduct()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3DUR2Dw0qBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('note_label.txt', 'w+', encoding='utf8')\n",
        "for i in range(500, 1000):\n",
        "  for s, l in get_predict(i):\n",
        "    file.writelines(str(i)+'\\t'+s+'\\t'+l+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}